#+SETUPFILE: ../style/default.setup
#+INCLUDE: ../style/latex.setup
#+TITLE: Mathamatical Tools

* Statistics

** Basic probability and statistics

*** Central limit theorem

Using the following code, I showed that the number of head-up coins of 100 flipping followes the following distribution: N(50, sigma^2) and sigma = 5. I will work out the math.

#+begin_src python :results file :export both :cache no
N = 10000
a = np.random.choice([0, 1], [N, 100])
count = np.sum(a, axis=1)
# bins = np.linspace(0, 100, 50)
# n, _ = np.histogram(count, bins=bins)
from scipy.stats import norm
mu, std = norm.fit(count)
#+end_src


** Bayesian Statistics

Here I describe the very basics of Bayes inference with some examples.
Here's an article about
[[http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/][How
to be a Bayesian in Python]], which introduces the Python MCMC package:
[[http://dfm.io/emcee/current/][emcee]].

*** Bayes' Theorem

$$P(A|B) = \frac{P(B|A)P(A)}{P(B)},
  \label{eq:bayes}$$ where Bayesian interpretations of the factors in
this equation are:

- $P(A|B)$ is the probability of the hypothesis given the dat and prior
  information.

- $P(B|A)$ is the probability that the data would be observed if the
  hypothesis were true.

- $P(A)$ is the prior probability of the hypothesis being true (in other
  words, the probability you assigned to the hypothesis before you took
  the data).

- $P(B)$ can be considered as a normalizing constant, given that
  probabilities must integrate to unity.

*** Parameter Estimation, Discrete Distributions

First consider the discrete case. In a particular model, you expect
there to be $m_i$ counts in bin $i$. Then, the probability of actually
observing $d_i$ counts in bin $i$ of the data, given the model is
correct, is, from the Poisson distribution,
$${\cal L}_i = \frac{m_i^{d_i}}{d_i!} e^{-m_i}.
  \label{eq:L}$$

In the limit of Gaussian statistics, the probability of the data given
the model in bin $i$, $p_i$, is given by
$$p_i = \frac{1}{\sigma_i \sqrt{2\pi}} e^{-(d_i - m_i)^2 / 2\sigma_i^2}.
  \label{eq:pi}$$ Wilks' theorem tells us that
$2\Delta \ln {\cal L} = - \Delta \chi^2$ in the Gaussian limit.
$1\sigma$ range would be $\Delta \chi^2 = 1$, which corresponds to
$\Delta \ln {\cal L} = -0.5$ around the maximum.

*** Parameter Estimation, Continuous Distributions

The probability of a data is simplily the probability distribution
function (PDF): $dP = N(x) \mathop{}\!\mathrm{d}x$, and the log
posterior probability $\ln {\cal L} =
\Sigma \ln N(x_i)$ + const, where $x_i$ is the observed data.

*** Nuisance Parameters: Marginalization

Given a posterio probability distribution $P(a_1, a_2, \cdots, a_n)$,
normalized so that\\
$\int P(a_{1}, a_{2}, \cdots, a_{n}) da_1 da_2 \cdots da_n=1$. To get
the probability distribution for parameter $a_1$, independent of the
values of the other parameters, we integrate over those other
parameters:
$$p(a_1) = \int P(a_1, a_2, \cdots, a_n) \mathop{}\!\mathrm{d}a_2 \cdots \mathop{}\!\mathrm{d}a_n.
  \label{eq:margin}$$

*** Model Comparison

"Bayes factor".

*** Example: Fitting a Straight Line to Data

Assume a fitting to the data $$y = \beta_0 + \beta_1 \, x. \label{eq:y}$$ If we assume that each data point departs from the staright line by an amount $\epsilon_i$ which as a $\mathcal{N}(0,\sigma^2)$ distribution, and $\sigma$ is known, i.e.,

$$y_i \sim {\cal N} (\beta_0 + \beta_1 x_i, \sigma^2),
  \label{eq:yi}$$

then, the posterior distribution for $\beta_0$ and $\beta_1$ is proportional to the product of the $N$ normal densities:

$$P(\beta_0,\beta_1|{y_1,y_2,\cdots,y_N}) \propto \prod_{i=1}^{N} \exp \left[
  -\frac{(y_i-(\beta_0+\beta_1 x_i))^2}{2\sigma^2} \right].
  \label{eq:pbeta}$$

In practice, we set the prior as $p(\theta, b) = p(\theta)p(b)$, where $p(\theta) = 1/\pi$ from $\theta=0$ to $\theta = \pi$ and 0 otherwise, and $p(b) = 1/(b_2 - b_1)$ from $b=b_1$ to $b=b_2$ and 0 otherwise. Then,

$$P(\theta, b) \propto \frac{1}{\pi} \frac{1}{b_2 - b_1} \prod_{i=1}^{N} \exp \left[ -\frac{(y_i-(\beta_0+\beta_1 x_i))^2}{2\sigma^2} \right]. \label{eq:pbeta2}$$

For data with uncertainties, $\sigma$ may be replaced with $\sigma_i$, the reported Gaussian uncertainty in the $i$th measurement of x. Note that the log of the product is $- \chi^2/2$. We then normalize the posterior probability density such that

$$\int_0^\pi \int_{b_1}^{b_2} P(\theta, b) \mathop{}\!\mathrm{d}b \mathop{}\!\mathrm{d}\theta = 1. \label{eq:norm}$$

** Two-point Correlation Function

[[http://www.astro.lu.se/Education/utb/ASTM21/ASTM21-P1.pdf][This pdf document]] (or [[file:~/Academics/Books-more/Astrophysics/StarsStarClusters/ASTM21-P1.pdf][local]]) introduces the theories of two-point correlation function with references. Python has a package to do pair counting.

*** Definition

In order to quantify the clustering of galaxies, one must survey the entire galaxy distribution, from voids to superclusters. The most commonly used quantitative measure of large scale structure is the galaxy two-point correlation function, which is the amplitude of galaxy clustering as a function of scale. The two-point correlation function, $\xi(r)$, is defined as a measure of the excess probability above what is expected for an unclustered random Poisson distribution, of finding a galaxy in a volume element $dV$ at a separation $r$ from another galaxy,

$$ dP = [1 + \xi(r)] n dV, $$

*** Estimators

The dataset ($D$) under investigation contains $n$ points. The two-point correlation function is obtained by comparing the distribution of separations in $D$ with that in a random set, called $R$, which contains $r$ points. In the interval $x$ to $x + \delta x$, count the number of pairs in $D$ and $R$ and denote them as $N_D$ and $N_R$, respectively. The 'natural' estimator of $w(x)$ is

$$w_1 = \frac{\frac{N_D}{n(n-1)/2} - \frac{N_R}{r(r-1)/2}}{\frac{N_R}{r(r-1)/2}} = \frac{r(r-1)}{n(n-1)} \frac{N_D}{N_R} - 1 \approx \frac{r^2}{n^2} \frac{N_D}{N_R} - 1.$$

Landy & Szalay suggested a much better estimate by using also the cross-correlation statistic $N_{DR}$, which is the number of pairs in the seperation interval, with one point taken from $D$ and the other from $R$. $$w_3 \approx \frac{N_D/n^2 - 2N_{DR}/nr + N_R/r^2}{N_R/r^2} = \frac{r^2 N_D}{n^2 N_R} - \frac{r N_{DR}}{n N_{R}} + 1.$$

*** Analytical Calculation of the Random Field

I propose to use a general analytical expression to account for Input

#+BEGIN_EXAMPLE
    Integrate[x ArcCos[(x^2 + r^2 - R^2)/(2 x r)], x]
#+END_EXAMPLE

Output

#+BEGIN_EXAMPLE
   1/4 x (-r Sqrt[-((r^4 + (R^2 - x^2)^2 - 2 r^2 (R^2 + x^2))/(
        r^2 x^2))] + 2 x ArcCos[(r^2 - R^2 + x^2)/(2 r x)]) - (
   r R^2 x Sqrt[-((r^4 + (R^2 - x^2)^2 - 2 r^2 (R^2 + x^2))/(r^2 x^2))]
     ArcTan[(r^2 + R^2 - x^2)/
     Sqrt[-r^4 - (R^2 - x^2)^2 + 2 r^2 (R^2 + x^2)]])/(
   2 Sqrt[-r^4 - (R^2 - x^2)^2 + 2 r^2 (R^2 + x^2)])
#+END_EXAMPLE

* Fourier Transforms

Define the Fourier transformation $$\label{eq:fourier1}
F(\omega) = \int_{-\infty}^{+\infty} e^{-i\omega\tau} f(\tau) \mathop{}\!\mathrm{d}\tau,$$
and the inverse transformation that relates $f(t)$ to $F(\tau)$,
$$\label{eq:6}
f(t) = \frac{1}{2\pi} \int_{-\infty}^{+\infty} e^{i \omega t}
F(\omega) \mathop{}\!\mathrm{d}\omega.$$ Note that $f(\tau)$ is usually
a real function. Physicists prefer the following form for convenience:

$$\begin{aligned}
\Phi(p) &= \int_{-\infty}^{+\infty} F(x) e^{-2\pi i p x} \mathop{}\!\mathrm{d}x,\\
F(x) &= \int_{-\infty}^{+\infty} \Phi(p) e^{2\pi i x p} \mathop{}\!\mathrm{d}p.
\label{eq:fourier2}\end{aligned}$$

Some popular pairs of conjugate variables, $x$ and $p$, are 1) time $t$
and frequency $\nu$, in accoustics and radio; 2) position $x$ and
momentum divided by Planck's constant $p/\hbar$ in quantum mechanics; 3)
aperture $x$ and diffraction angle divied by wavelength $\sin
\theta/\lambda$ in diffraction theory.

I list the Fourier Transforms of some of the commonly used functions in
# Tab.Â [[tab:1][1]]

**** Power spectrum

$S(\nu) = \Phi(\nu) \Phi^{*}(\nu) = \left| \Phi(\nu) \right|^2$ is
called the power spectrum of the spectral power density (SPD) of $F(t)$.
This is what an optical spectrometer measures.


| $F(x)$                        | $\Phi(p)$                              | Notes                                                                                       |
|-------------------------------+----------------------------------------+---------------------------------------------------------------------------------------------|
| 'top-hat' function $\Pi_a(x)$ | $a~\mathrm{sinc}(\pi p a)$             |                                                                                             |
| Gaussian $G(x)=e^{-x^2/a^2}$  | $g(p) = a \sqrt{\pi} e^{-\pi^2a^2p^2}$ | $G(x)$ is a Gausian with width parameter $a$, while $g(p)$ has a width parameter $1/\pi a$. |
|                               |                                        |                                                                                             |
| $f(x) = e^{-x/a} \; (x>0)$    | $\Phi(p) = \frac{-1}{2\pi i p -1/a}$   | The power spectrum of $f(x)$ is a Lorentz profile with a FWHM $1/\pi a$                     |
| $\delta(x)$                   | 1                                      | "Small is big; big is small." Think about a Gaussian function with unit area.               |
#+CAPTION: <<tab:1>> A table of Fourier transform functions.

* Fitting functions
  :PROPERTIES:
  :CUSTOM_ID: sec:math
  :END:

** ArcTan and Sigmoid function
   :PROPERTIES:
   :CUSTOM_ID: subsec:fermi
   :END:

The Fermi Function is used to fit the SFE(t) in our project:
$$\label{eq:3}
f(x) = \frac{a}{e^{-(x-\mu)/k} + 1}.$$ Its derivative
$F(x) = \d f(x) / \d x$ has a smilar shape as the Gaussian function. It
peaks at $x=\mu$ at which $F(x) = a/4k$. Its Full-Width Half-Maximum
(FWHM) is $2 \ln(3+2 \sqrt{2}) k \approx
3.525 k$.

The $\arctan$ function $f(x) = a \arctan(k(x-b))$ has the similar shape
with its derivative having a height of $a k$ and FWHM of $2/k$.

